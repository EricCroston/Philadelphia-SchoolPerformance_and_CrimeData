


import warnings
import pandas as pd
from pathlib import Path
import geopandas





# File Path to Police Incidents in Philadelphia from January 2024- July 2024
incidents_data_to_load = Path("data\incidents_part1_part2.csv")

# Read Police Incidents into Pandas DataFrames
incidents_data = pd.read_csv(incidents_data_to_load)

# Check the DataFrame
incidents_data.head()


# Inspect Incident Data
incidents_data.info()





# Calculate the number of unique general codes and UCR codes for incident type
text_general_code_count = incidents_data['text_general_code'].nunique()
ucr_general_count = incidents_data['ucr_general'].nunique()

print("The number of unique general codes for incident type:", text_general_code_count)
print("The number of unique general UCR codes for incident type:", ucr_general_count)

# Group incidents by their UCR code and list them
ucr_incidents = incidents_data.groupby('ucr_general')['text_general_code'].unique()

# Convert the groupby object to a DataFrame for better readability
ucr_incidents_df = ucr_incidents.reset_index()
ucr_incidents_df.columns = ['ucr_general_code', 'incidents']

ucr_incidents_df


# Calculate the number of unique general codes and UCR codes for incident type
dc_dist_count = incidents_data['dc_dist'].nunique()
psa_count = incidents_data['psa'].nunique()
objectid_count = incidents_data['objectid'].nunique()
dc_key_count = incidents_data['dc_key'].nunique()

print("The number of police districts:", dc_dist_count)
print("The number of police service areas:", psa_count)
print("The number of object ids for incidents:", objectid_count)
print("The number dc_key_counts for incidents:", dc_key_count)








# Identify duplicates based on 'dc_key' 
duplicates = incidents_data[incidents_data.duplicated(subset=['dc_key'], keep=False)]

print("Duplicates based on 'dc_key':")
duplicates
# duplicates.count()





# Convert 'dispatch_date_time' to datetime format
incidents_data['dispatch_date_time'] = pd.to_datetime(incidents_data['dispatch_date_time'], errors='coerce')

# Convert 'dispatch_date' to datetime format (date only)
incidents_data['dispatch_date'] = pd.to_datetime(incidents_data['dispatch_date'], format='%Y-%m-%d', errors='coerce')

# Convert 'dispatch_time' to datetime format (time only)
incidents_data['dispatch_time'] = pd.to_datetime(incidents_data['dispatch_time'], format='%H:%M:%S', errors='coerce').dt.time

# Check the DataFrame to ensure the conversion
print(incidents_data[['dispatch_date_time', 'dispatch_date', 'dispatch_time']].head())

incidents_data.info()





# Creating the combined key
incidents_data['district_psa'] = incidents_data['dc_dist'].astype(str).str.zfill(2) + incidents_data['psa'].astype(str)

incidents_data = incidents_data[incidents_data['district_psa']!='77A']

incidents_data = incidents_data.reset_index(drop=True)

# Display the result
incidents_data.head()





#Make a new data frame for Incident Information and Removing Columns and Rows with NaN
incidents_data_df = pd.DataFrame({
    'incident_id': incidents_data['objectid'],
    'district': incidents_data['dc_dist'],
    'police_service_area': incidents_data['psa'], 
    'district_psa': incidents_data['district_psa'], 
    'dispatch_date_and_time': incidents_data['dispatch_date_time'],
    'dispatch_date': incidents_data['dispatch_date'],
    'dispatch_time': incidents_data['dispatch_time'],      
    'dispatch_hour': incidents_data['hour'],
    'location_block': incidents_data['location_block'],
    'ucr_code': incidents_data['ucr_general'],
    'general_crime_category': incidents_data['text_general_code']
})

incidents_data_df.head()


# Drop rows with any NaN values
incidents_data_df = incidents_data_df.dropna(how='any')

# Drop rows where 'Not Available' is present in any column
incidents_data_df = incidents_data_df[~incidents_data_df.isin(['Not Available']).any(axis=1)]

columns_to_convert = [
    'incident_id',
    'district',
    'police_service_area',
    'district_psa',
    'dispatch_date_and_time',
    'dispatch_date',
    'dispatch_time',
    'dispatch_hour',
    'location_block',
    'ucr_code',
    'general_crime_category'
]

# Copy the original DataFrame
incidents_time_place_df = incidents_data_df.copy()  

incidents_time_place_df.head()





district_psa_df=pd.DataFrame({
    'district_psa':incidents_time_place_df['district_psa'],
    'district':incidents_time_place_df['district'],
    'police_service_area':incidents_time_place_df['police_service_area']
})

# Sort by 'District' first and then by 'Police Service Area'
district_psa_df = district_psa_df.sort_values(by=['district', 'police_service_area'])

# Remove duplicates based on a specific column or combination of columns
district_psa_df = district_psa_df.drop_duplicates(subset=['district_psa'], keep='first')

# Reset the index if needed
district_psa_df.reset_index(drop=True, inplace=True)

# Save the cleaned DataFrame to a new CSV file
output_file_path = 'jupyter_outputs/District_PSA.csv'

# Save the DataFrame to a CSV file
district_psa_df.to_csv(output_file_path, index=False)

print(f"DataFrame saved to {output_file_path}")
district_psa_df.head()





# Create the 'incident_code_df' dataframe
incident_code_df=pd.DataFrame({
    'ucr_code':incidents_data['ucr_general'],
    'general_crime_category':incidents_data['text_general_code']
})

# Sort by 'UCR Code' first and then by 'General Crime Category'
incident_code_df = incident_code_df.sort_values(by=['ucr_code'])

# Reset index if you want a clean index
incident_code_df = incident_code_df.reset_index(drop=True)

incident_code_df.head()


# Define the crime groups
group_mapping = {
    'Violent_Crimes': [
        'Homicide - Criminal', 'Homicide - Justifiable', 'Rape', 'Robbery No Firearm', 
        'Robbery Firearm', 'Aggravated Assault Firearm', 'Aggravated Assault No Firearm', 
        'Other Assaults'
    ],
    'Property_Crimes': [
        'Burglary Residential', 'Burglary Non-Residential', 'Thefts', 
        'Theft from Vehicle', 'Motor Vehicle Theft', 'Arson', 
        'Forgery and Counterfeiting', 'Receiving Stolen Property', 
        'Vandalism/Criminal Mischief'
    ],
    'Public_Order_Crimes': [
        'Disorderly Conduct', 'Vagrancy/Loitering', 'Public Drunkenness', 
        'Liquor Law Violations', 'DRIVING UNDER THE INFLUENCE'
    ],
    'Drug_and_Vice_Crimes': [
        'Prostitution and Commercialized Vice', 'Narcotic / Drug Law Violations', 
        'Other Sex Offenses (Not Commercialized)', 'Gambling Violations'
    ],
    'Other_Crimes': [
        'Offenses Against Family and Children', 'Fraud', 'Embezzlement', 
        'Weapon Violations', 'All Other Offenses'
    ]
}

# Reverse the mapping to map each crime category to its group
crime_to_group = {crime: group for group, crimes in group_mapping.items() for crime in crimes}

# Create a new column for the crime group
incidents_time_place_df['crime_group'] = incidents_time_place_df['general_crime_category'].map(crime_to_group)

incidents_time_place_df.head()





# Make an output for the new dataset
output_file_path = 'jupyter_outputs/Incidents_Time_Place.csv'

# Save the DataFrame to a CSV file
incidents_time_place_df.to_csv(output_file_path, index=False)
print(f"DataFrame saved to {output_file_path}")





# Group by 'UCR Code' and aggregate unique values of 'General Crime Category' and 'Crime Group'
incidents_df = incidents_time_place_df.groupby('ucr_code').agg({'general_crime_category': lambda x: x.unique(), 'crime_group': lambda x: x.unique()})

# Convert the series to a DataFrame
incidents_df = incidents_df.reset_index()

# Make an output for the new dataset
output_file_path = 'jupyter_outputs/Incidents.csv'

# Save the DataFrame to a CSV file
incidents_df.to_csv(output_file_path, index=False)
print(f"DataFrame saved to {output_file_path}")

incidents_df.head()





import geopandas as gpd
from shapely.geometry import Point

# Load school locations
school_gdf = gpd.read_file('data\Schools.geojson')

# Load PSA boundaries
psa_gdf = gpd.read_file('data\Boundaries_PSA.geojson')

# Ensure both GeoDataFrames are in the same CRS
school_gdf = school_gdf.to_crs(epsg=4326)
psa_gdf = psa_gdf.to_crs(epsg=4326)

# Perform a spatial join to find which PSA polygon each school is in
school_with_psa_gdf = gpd.sjoin(school_gdf, psa_gdf, how='left', predicate='within')

# Extract relevant PSA and district information
school_with_psa_gdf['PSA_NUM'] = school_with_psa_gdf['PSA_NUM']
school_with_psa_gdf['DISTRICT'] = school_with_psa_gdf['DISTRICT__']

# Save the result to a new file or use it as needed
school_with_psa_gdf.to_file('jupyter_outputs/schools_with_psa_info.geojson', driver='GeoJSON')

school_with_psa_gdf.head()


school_with_psa_gdf.info()





# Rename the specified columns
school_with_psa_gdf = school_with_psa_gdf.rename(columns={
    'LOCATION_ID': 'school_id', 
    'SCHOOL_NAME': 'school_name', 
    'SCHOOL_NAME_LABEL': 'school_name_label',
    'STREET_ADDRESS': 'street_address',
    'ZIP_CODE': 'zip_code',
    'PHONE_NUMBER': 'phone_number',
    'GRADE_LEVEL': 'grade_level',
    'GRADE_ORG': 'grade_org',
    'TYPE': 'type',
    'TYPE_SPECIFIC': 'type_specific',
    'DISTRICT': 'district', 
    'PSA_NUM':'district_psa'
})

# List of columns to include, including the geometry column
columns_schools_with_geometry = [
    'school_id',
    'school_name',
    'school_name_label',
    'street_address',
    'zip_code',
    'phone_number',
    'grade_level',
    'grade_org',
    'type',
    'type_specific',
    'geometry',  
    'district', 
    'district_psa'
]

# Create 'school_info_df with the specified columns
school_info_df = school_with_psa_gdf[columns_schools_with_geometry]

# Drop rows where 'school_id' is None
school_info_df = school_info_df[school_info_df['school_id'].notna()]

# Drop rows with any NaN values
school_info_df = school_info_df.dropna(how='any')

# Remove duplicates based on 'school_id'
school_info_df = school_info_df.drop_duplicates(subset=['school_id'], keep='first')

# Strip unwanted characters and convert to integer
school_info_df['school_id'] = school_info_df['school_id'].astype(str).str.strip().str.replace(r'\r\n', '', regex=True).astype(int)

# Reset index for a clean index
school_info_df = school_info_df.reset_index(drop=True)

# Define the output file path
output_file_path = 'jupyter_outputs/School_Info.csv'

# Save the DataFrame to a CSV file
school_info_df.to_csv(output_file_path, index=False)

print(f"DataFrame saved to {output_file_path}")

school_info_df.head()


# Read the GeoJSON file into a GeoDataFrame
geo_police_gdf = gpd.read_file('data/Police_Stations.geojson')

geo_police_gdf.info()


# Rename the specified columns
geo_police_gdf = geo_police_gdf.rename(columns={
    'OBJECTID': 'object_id', 
    'DISTRICT_NUMBER': 'district_number', 
    'LOCATION': 'location',
    'TELEPHONE_NUMBER': 'phone_number',
    'RULEID': 'zip_code'
})

geo_police_gdf


# Create a list of zip codes for philly schools and police
zip_codes = [
    19115,
    19144,
    19114,
    19149,
    19141,
    19149,
    19128,
    19140,
    19124,
    19121,
    19124,
    19125,
    19151,
    19104,
    19130,
    19107,
    19143,
    19146,
    19147,
    19142,
    19145,
    19130,
    19106,
    19104
]

# Add the 'zip_code' column to the DataFrame
geo_police_gdf['zip_code'] = zip_codes[:len(geo_police_gdf)]

police_location_df = pd.DataFrame({
    'district': geo_police_gdf['district_number'],
    'location': geo_police_gdf['location'],  
    'zip_code': geo_police_gdf['zip_code'],
    'phone_number': geo_police_gdf['phone_number'],
    'geometry': geo_police_gdf['geometry']
})

# Sort the dataframe by the 'district' column in ascending order
police_location_df = police_location_df.sort_values(by='district', ascending=True).reset_index(drop=True)

# Make an output for the new dataset
output_file_path= 'jupyter_outputs/Police_Location.csv'

# Save the DataFrame to a CSV file
police_location_df.to_csv(output_file_path, index=False)

print(f"DataFrame saved to {output_file_path}")

police_location_df





police_districts_df = pd.merge(district_psa_df, police_location_df, on='district', how='left')

police_districts_df


# Make an output for the new dataset
output_file_path= 'jupyter_outputs/Police_Districts.csv'

# Save the DataFrame to a CSV file
police_districts_df.to_csv(output_file_path, index=False)

print(f"DataFrame saved to {output_file_path}")


# File Path to Total Crimes and Elementray School Data for Police Service Area
comparison_data_to_load = Path("SQL Outputs\Total_Crimes_ES_PSA.csv")

# Read Police Incidents Data File and store into Pandas DataFrames
comparison_data = pd.read_csv(comparison_data_to_load)

# Check DataFrame was created
comparison_data.head()


# Calculate correlation matrix
correlation_matrix = comparison_data.corr()
correlation_matrix


import matplotlib.pyplot as plt
import seaborn as sns

# Plotting the relationship between ELA Proficiency and Total Crimes
sns.regplot(x='Average_ELA_Proficiency_3_to_8', y='Total_Crimes', data=comparison_data)
plt.title('ELA Proficiency vs Total Crimes')
plt.show()




